{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "def claim_reader(raw):\n",
    "    #extract claim, people IDs and place IDs from quote chunk\n",
    "    metadata = {\n",
    "                'speaker': '',\n",
    "                'named_people': [],\n",
    "                'geographies': []}\n",
    "    claim = ''\n",
    "    extra = ''\n",
    "    if raw.count('\"') == 2:\n",
    "        parts = raw.split('\"')\n",
    "        claim = parts[1]\n",
    "        extra = \" \".join((parts[0], parts[2]))\n",
    "    else:\n",
    "        claim = raw\n",
    "    return claim   \n",
    "\n",
    "def clean_tweet_stub(raw):\n",
    "    #Remove trailing retweet/like data from Google stub for tweet listing\n",
    "    idx_string = \" Retweets(.*); Likes\"\n",
    "    compiled = re.compile(idx_string)\n",
    "    search_str = compiled.search(raw)\n",
    "    search_str = search_str.group(1).strip()\n",
    "    index = raw.index(\" Retweets %s; Likes\" % search_str)\n",
    "    tweet_content = raw[:index]\n",
    "    return tweet_content\n",
    "\n",
    "def tag_pos(sentence):\n",
    "    words = tokenize(sentence)\n",
    "    words = [nltk.pos_tag(words) for word in words]\n",
    "    return words\n",
    "    \n",
    "def ID_people(text):\n",
    "    pass\n",
    "\n",
    "def ID_geog(text):\n",
    "    pass\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text, stem=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if stem:\n",
    "        tokens = stem_tokens(tokens, stemmer)\n",
    "    return tokens\n",
    "\n",
    "def tf_idf_cosine_comparison (raw_text):\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    tfidf = vect.fit_transform(raw_text)\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "\n",
    "def google_claim (claim):\n",
    "    claim_google = claim.replace('\"', '%22')\n",
    "    url = 'https://www.google.com/search?q='+claim_google+'&aqs=chrome.2.69i57j69i65j0l4.5769j0j4&sourceid=chrome&ie=UTF-8'\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "    claim = claim_reader(claim)\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find_all(\"div\", {\"class\":\"g\"})\n",
    "    google_links = []\n",
    "    google_sources = []\n",
    "    google_headlines = []\n",
    "    google_stubs = []\n",
    "    twitter_users = []\n",
    "\n",
    "    for result in results:\n",
    "        link = result.find_all(\"a\")[0].get(\"href\")\n",
    "        #Get rid of the junk characters that prefix links\n",
    "        link = link[7:]\n",
    "        google_links.append(link)\n",
    "\n",
    "        headline = result.find_all(\"a\")[0].text\n",
    "        google_headlines.append(headline)\n",
    "\n",
    "        stub = result.find_all(\"span\", {\"class\":\"st\"})[0].text\n",
    "        google_stubs.append(stub)\n",
    "\n",
    "        index = link.index('//')+2\n",
    "        source_link = link[index:]\n",
    "        end_point = source_link.index('/')\n",
    "        domain = source_link[:end_point]\n",
    "        if \"twitter.com\" in domain:\n",
    "            un_start = source_link.index('.com/')+5\n",
    "            un = source_link[un_start:]\n",
    "            un_end = un.index('/')\n",
    "            un = un[:un_end]\n",
    "            twitter_users.append(un)\n",
    "            stub = clean_tweet_stub(stub)\n",
    "            #Call comparison function - ADD IF STATEMENT TO VERIFY PERSON CLAIMING IS CORRECT\n",
    "            similarity_score = tf_idf_cosine_comparison([claim, stub])\n",
    "            print(\"Claim: \",claim)\n",
    "            print(\"Stub: \",stub)\n",
    "            print(\"Similarity score:\", similarity_score)\n",
    "            \n",
    "        google_sources.append(domain)\n",
    "\n",
    "def google_claim (claim):\n",
    "    claim_twitter = claim.replace('\"', '%22')\n",
    "    url='https://twitter.com/search?q='+claim_twitter\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "    claim = claim_reader(claim)\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find_all(\"div\", {\"class\":\"stream\"})\n",
    "    google_links = []\n",
    "    google_sources = []\n",
    "    google_headlines = []\n",
    "    google_stubs = []\n",
    "    twitter_users = []\n",
    "\n",
    "    for result in results:\n",
    "        link = result.find_all(\"a\")[0].get(\"href\")\n",
    "        #Get rid of the junk characters that prefix links\n",
    "        link = link[7:]\n",
    "        google_links.append(link)\n",
    "\n",
    "        headline = result.find_all(\"a\")[0].text\n",
    "        google_headlines.append(headline)\n",
    "\n",
    "        stub = result.find_all(\"span\", {\"class\":\"st\"})[0].text\n",
    "        google_stubs.append(stub)\n",
    "\n",
    "        index = link.index('//')+2\n",
    "        source_link = link[index:]\n",
    "        end_point = source_link.index('/')\n",
    "        domain = source_link[:end_point]\n",
    "        if \"twitter.com\" in domain:\n",
    "            un_start = source_link.index('.com/')+5\n",
    "            un = source_link[un_start:]\n",
    "            un_end = un.index('/')\n",
    "            un = un[:un_end]\n",
    "            twitter_users.append(un)\n",
    "            stub = clean_tweet_stub(stub)\n",
    "            #Call comparison function - ADD IF STATEMENT TO VERIFY PERSON CLAIMING IS CORRECT\n",
    "            similarity_score = tf_idf_cosine_comparison([claim, stub])\n",
    "            print(\"Claim: \",claim)\n",
    "            print(\"Stub: \",stub)\n",
    "            print(\"Similarity score:\", similarity_score)\n",
    "\n",
    "        google_sources.append(domain)\n",
    "\n",
    "claim = '“global warming was created by the Chinese to make US manufacturing non-competitive.” Donald Trump'.replace('”', '\"').replace('“', '\"')\n",
    "\n",
    "google_claim(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def source_quote_eventregistry(quote):\n",
    "    # Setup the event registry access\n",
    "    er = EventRegistry()\n",
    "       \n",
    "    print('\\n\\nClaim is: '+quote)\n",
    "    # need to shorten the claim to 15 words or less for free account\n",
    "    quote=quote.split()[:15]\n",
    "    print('Truncated claim is: '+quote)\n",
    "    q = QueryArticles(keywords = quote)\n",
    "    q.addRequestedResult(RequestArticlesInfo())\n",
    "\n",
    "    known_sources=['bbc.co.uk','guardian.com','reuters.com','theherald.com']\n",
    "    # JSON return data from event registry query - just grabbing the first instance here, but potentially we could\n",
    "    # filter by news source - say we know how to parse BBC/guardian/etc pages and we have confidence in their accuracy\n",
    "    jsonResults=er.execQuery(q)\n",
    "\n",
    "    print json.dumps(er.execQuery(q), sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    #url=json.dumps(jsonResults[\"articles\"][\"results\"][0][\"url\"])[1:-1]\n",
    "    sources=[]\n",
    "    # if there were some results\n",
    "    if not jsonResults[\"error\"]:\n",
    "        for article in jsonResults[\"articles\"][\"results\"]:\n",
    "            # we've now got the article url from result set, check if it's in the known sources list\n",
    "            sources.append(json.dumps(article[\"url\"])[1:-1])\n",
    "\n",
    "#             for known_source in known_sources:\n",
    "#                 if known_source in json.dumps(article[\"url\"])[1:-1]:\n",
    "#                     sources.append(json.dumps(article[\"url\"])[1:-1])\n",
    "#                     break\n",
    "\n",
    "    if sources:\n",
    "        print(\"Found \"+str(len(sources))+\" sources\")\n",
    "        for url in sources:\n",
    "            print(\"Checking: \"+url)\n",
    "            # use this url to grab the original article\n",
    "            headers = {'user-agent': 'Mozilla/5.0'}\n",
    "            r = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "\n",
    "            # the original article text\n",
    "            articleText=soup.getText().encode('utf-8').strip()\n",
    "            #print(articleText)\n",
    "\n",
    "            # the most primitive form of checking if the claim is in the article:\n",
    "            if claim in str(articleText):\n",
    "                print(\"Claim is present in article\")\n",
    "                # break out of searching the sources when we find a valid one\n",
    "                return url\n",
    "                break\n",
    "            else:\n",
    "                print(\"Claim doesn't appear\")\n",
    "    else:\n",
    "        # if there are no accepted sources\n",
    "        print(\"Couldn't find a source for this quote\")\n",
    "\n",
    "with open('sampledata.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        source_quote_eventregistry(row[1])\n",
    "        #source_quote_eventregistry('The concept of global warming was created by and for the Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
