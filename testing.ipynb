{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "def claim_reader(raw):\n",
    "    #extract claim, people IDs and place IDs from quote chunk\n",
    "    metadata = {\n",
    "                'speaker': '',\n",
    "                'named_people': [],\n",
    "                'geographies': []}\n",
    "    claim = ''\n",
    "    extra = ''\n",
    "    if raw.count('\"') == 2:\n",
    "        parts = raw.split('\"')\n",
    "        claim = parts[1]\n",
    "        extra = \" \".join((parts[0], parts[2]))\n",
    "    else:\n",
    "        claim = raw\n",
    "    return claim   \n",
    "\n",
    "def clean_tweet_stub(raw):\n",
    "    #Remove trailing retweet/like data from Google stub for tweet listing\n",
    "    idx_string = \" Retweets(.*); Likes\"\n",
    "    compiled = re.compile(idx_string)\n",
    "    search_str = compiled.search(raw)\n",
    "    search_str = search_str.group(1).strip()\n",
    "    index = raw.index(\" Retweets %s; Likes\" % search_str)\n",
    "    tweet_content = raw[:index]\n",
    "    return tweet_content\n",
    "\n",
    "def tag_pos(sentence):\n",
    "    words = tokenize(sentence)\n",
    "    words = [nltk.pos_tag(words) for word in words]\n",
    "    return words\n",
    "    \n",
    "def ID_people(text):\n",
    "    pass\n",
    "\n",
    "def ID_geog(text):\n",
    "    pass\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text, stem=False):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    if stem:\n",
    "        tokens = stem_tokens(tokens, stemmer)\n",
    "    return tokens\n",
    "\n",
    "def tf_idf_cosine_comparison (raw_text):\n",
    "    vect = TfidfVectorizer(min_df=1)\n",
    "    tfidf = vect.fit_transform(raw_text)\n",
    "    return (tfidf * tfidf.T).A[0,1]\n",
    "\n",
    "def google_claim (claim):\n",
    "    claim_google = claim.replace('\"', '%22')\n",
    "    url = 'https://www.google.com/search?q='+claim_google+'&aqs=chrome.2.69i57j69i65j0l4.5769j0j4&sourceid=chrome&ie=UTF-8'\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "    claim = claim_reader(claim)\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find_all(\"div\", {\"class\":\"g\"})\n",
    "    google_links = []\n",
    "    google_sources = []\n",
    "    google_headlines = []\n",
    "    google_stubs = []\n",
    "    twitter_users = []\n",
    "\n",
    "    for result in results:\n",
    "        link = result.find_all(\"a\")[0].get(\"href\")\n",
    "        #Get rid of the junk characters that prefix links\n",
    "        link = link[7:]\n",
    "        google_links.append(link)\n",
    "\n",
    "        headline = result.find_all(\"a\")[0].text\n",
    "        google_headlines.append(headline)\n",
    "\n",
    "        stub = result.find_all(\"span\", {\"class\":\"st\"})[0].text\n",
    "        google_stubs.append(stub)\n",
    "\n",
    "        index = link.index('//')+2\n",
    "        source_link = link[index:]\n",
    "        end_point = source_link.index('/')\n",
    "        domain = source_link[:end_point]\n",
    "        if \"twitter.com\" in domain:\n",
    "            un_start = source_link.index('.com/')+5\n",
    "            un = source_link[un_start:]\n",
    "            un_end = un.index('/')\n",
    "            un = un[:un_end]\n",
    "            twitter_users.append(un)\n",
    "            stub = clean_tweet_stub(stub)\n",
    "            #Call comparison function - ADD IF STATEMENT TO VERIFY PERSON CLAIMING IS CORRECT\n",
    "            similarity_score = tf_idf_cosine_comparison([claim, stub])\n",
    "            print(\"Claim: \",claim)\n",
    "            print(\"Stub: \",stub)\n",
    "            print(\"Similarity score:\", similarity_score)\n",
    "            \n",
    "        google_sources.append(domain)\n",
    "\n",
    "def google_claim (claim):\n",
    "    claim_twitter = claim.replace('\"', '%22')\n",
    "    url='https://twitter.com/search?q='+claim_twitter\n",
    "    headers = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "    claim = claim_reader(claim)\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find_all(\"div\", {\"class\":\"stream\"})\n",
    "    google_links = []\n",
    "    google_sources = []\n",
    "    google_headlines = []\n",
    "    google_stubs = []\n",
    "    twitter_users = []\n",
    "\n",
    "    for result in results:\n",
    "        link = result.find_all(\"a\")[0].get(\"href\")\n",
    "        #Get rid of the junk characters that prefix links\n",
    "        link = link[7:]\n",
    "        google_links.append(link)\n",
    "\n",
    "        headline = result.find_all(\"a\")[0].text\n",
    "        google_headlines.append(headline)\n",
    "\n",
    "        stub = result.find_all(\"span\", {\"class\":\"st\"})[0].text\n",
    "        google_stubs.append(stub)\n",
    "\n",
    "        index = link.index('//')+2\n",
    "        source_link = link[index:]\n",
    "        end_point = source_link.index('/')\n",
    "        domain = source_link[:end_point]\n",
    "        if \"twitter.com\" in domain:\n",
    "            un_start = source_link.index('.com/')+5\n",
    "            un = source_link[un_start:]\n",
    "            un_end = un.index('/')\n",
    "            un = un[:un_end]\n",
    "            twitter_users.append(un)\n",
    "            stub = clean_tweet_stub(stub)\n",
    "            #Call comparison function - ADD IF STATEMENT TO VERIFY PERSON CLAIMING IS CORRECT\n",
    "            similarity_score = tf_idf_cosine_comparison([claim, stub])\n",
    "            print(\"Claim: \",claim)\n",
    "            print(\"Stub: \",stub)\n",
    "            print(\"Similarity score:\", similarity_score)\n",
    "\n",
    "        google_sources.append(domain)\n",
    "\n",
    "claim = '“global warming was created by the Chinese to make US manufacturing non-competitive.” Donald Trump'.replace('”', '\"').replace('“', '\"')\n",
    "\n",
    "google_claim(claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from eventregistry import *\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def source_quote_eventregistry(quote):\n",
    "    # Setup the event registry access\n",
    "    er = EventRegistry()\n",
    "       \n",
    "    print('\\n\\nClaim is: '+quote)\n",
    "    # need to shorten the claim to 15 words or less for free account\n",
    "    quote=quote.split()[:15]\n",
    "    print('Truncated claim is: '+quote)\n",
    "    q = QueryArticles(keywords = quote)\n",
    "    q.addRequestedResult(RequestArticlesInfo())\n",
    "\n",
    "    known_sources=['bbc.co.uk','guardian.com','reuters.com','theherald.com']\n",
    "    # JSON return data from event registry query - just grabbing the first instance here, but potentially we could\n",
    "    # filter by news source - say we know how to parse BBC/guardian/etc pages and we have confidence in their accuracy\n",
    "    jsonResults=er.execQuery(q)\n",
    "\n",
    "    print json.dumps(er.execQuery(q), sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    #url=json.dumps(jsonResults[\"articles\"][\"results\"][0][\"url\"])[1:-1]\n",
    "    sources=[]\n",
    "    # if there were some results\n",
    "    if not jsonResults[\"error\"]:\n",
    "        for article in jsonResults[\"articles\"][\"results\"]:\n",
    "            # we've now got the article url from result set, check if it's in the known sources list\n",
    "            sources.append(json.dumps(article[\"url\"])[1:-1])\n",
    "\n",
    "#             for known_source in known_sources:\n",
    "#                 if known_source in json.dumps(article[\"url\"])[1:-1]:\n",
    "#                     sources.append(json.dumps(article[\"url\"])[1:-1])\n",
    "#                     break\n",
    "\n",
    "    if sources:\n",
    "        print(\"Found \"+str(len(sources))+\" sources\")\n",
    "        for url in sources:\n",
    "            print(\"Checking: \"+url)\n",
    "            # use this url to grab the original article\n",
    "            headers = {'user-agent': 'Mozilla/5.0'}\n",
    "            r = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "            [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "\n",
    "            # the original article text\n",
    "            articleText=soup.getText().encode('utf-8').strip()\n",
    "            #print(articleText)\n",
    "\n",
    "            # the most primitive form of checking if the claim is in the article:\n",
    "            if claim in str(articleText):\n",
    "                print(\"Claim is present in article\")\n",
    "                # break out of searching the sources when we find a valid one\n",
    "                return url\n",
    "                break\n",
    "            else:\n",
    "                print(\"Claim doesn't appear\")\n",
    "    else:\n",
    "        # if there are no accepted sources\n",
    "        print(\"Couldn't find a source for this quote\")\n",
    "\n",
    "with open('sampledata.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        source_quote_eventregistry(row[1])\n",
    "        #source_quote_eventregistry('The concept of global warming was created by and for the Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "# takes a text name and searches the google knowledge tree for the top hit\n",
    "# then returns the url of their wikipedia page and their schema.org id\n",
    "\n",
    "def find_person(name):\n",
    "    api_key = open('.api_key').read()\n",
    "    #api_key='AIzaSyBIE1JFd1qUFpSIkD4fPUdlq5xPR3jQjP4'\n",
    "    query = 'Donald Trump'\n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'limit': 10,\n",
    "        'indent': True,\n",
    "        'key': api_key,\n",
    "    }\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    response = json.loads(urllib.urlopen(url).read())\n",
    "    #if 'Person' in response['itemListElement']['@type']:\n",
    "    #    print 'person query!'\n",
    "    wiki_url=''\n",
    "    schema_id=''\n",
    "    for element in response['itemListElement']:\n",
    "        if 'Person' in element['result']['@type']:\n",
    "            print 'got person'\n",
    "            wiki_url=element['result']['detailedDescription']['url']\n",
    "            schema_id=element['result']['@id']\n",
    "            break # after the top hit\n",
    "    #     print element['result']['name'] + ' (' + str(element['resultScore']) + ')'\n",
    "    # print json.dumps(response, indent=4, sort_keys=True)\n",
    "    return wiki_url, schema_id\n",
    "\n",
    "find_person('Donald Trump')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# A function to search a wikipedia page for a phrase\n",
    "def search_for_term(url,term):\n",
    "    respond = requests.get(url)\n",
    "    soup = BeautifulSoup(respond.text)\n",
    "    content_text = soup.find(id=\"content\").text\n",
    "    # if phrase is found\n",
    "    if term in content_text:\n",
    "        print 'found'\n",
    "    \n",
    "url='https://en.wikipedia.org/wiki/Donald_Trump'\n",
    "search_for_term(url,'global warming')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.faroo.com/api?q=' + 'Donald%20Trump' + 'start=1&length=10&l=en&src=news&f=json'\n",
    "response = json.loads(urllib.urlopen(url).read())\n",
    "print json.dumps(response, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"NBC pulls Donald Trump-inspired 'Law &amp; Order: SVU' episode until after election https://t.co/Irk0o0yDd3\",\n",
       " u'#SM Howard Stern Explains Why He Won\\u2019t \\u2018Betray\\u2019 Donald Trump By Replaying Old Interviews\\u2026 https://t.co/OANuG6qWoL',\n",
       " u'#SM Taylor Swift Won&amp;rsquo;t Save Us From Donald Trump: As Donald J. Trump creeps (literally\\u2026 https://t.co/R1bVNAiILh',\n",
       " u'Melania Trump Defends Husband\\u2019s Attacks on Bill Clinton\\u2019s Past: \\u2018They\\u2019re Asking for It\\u2019: In\\u2026 https://t.co/vt13llUBcs',\n",
       " u'#SM Amber Tamblyn Channels Donald Trump To Perform Color Me Badd In This \\u2018Lip Sync Battle\\u2026 https://t.co/w4mkXXe0TJ',\n",
       " u'Amy Schumer -- Booed by Donald Trump Fans at Tampa Show (VIDEO): Amy Schumer made the\\u2026 https://t.co/qMmPacsHAH',\n",
       " u'#SM Alec Baldwin Is Basically a Teeny, Tiny Donald Trump in Dreamworks\\u2019 The Boss Baby\\u2026 https://t.co/xQaNXkXCnp',\n",
       " u'Amber Tamblyn Mocks Donald Trump on \\u2018Lip Sync Battle\\u2019: Amber Tamblyn dons a golden wig and\\u2026 https://t.co/vTs8fgYUac',\n",
       " u'RT @washingtonpost: No candidate in recorded history has overcome Trump\\u2019s current poll deficit this late https://t.co/ajSp0rFG1B',\n",
       " u'#SM Elizabeth Warren Taunts Donald Trump By Clucking Like A Chicken Over His Missing Tax\\u2026 https://t.co/gdEVGZ93Yx',\n",
       " u'#SM What did Donald Trump call Khloe Kardashian during \\u2018Celebrity Apprentice\\u2019? It\\u2019s\\u2026 https://t.co/yN6TCoClQh',\n",
       " u'#SM You Can Never Unsee Amber Tamblyn in Donald Trump Drag Giving America Ferrera a Lap\\u2026 https://t.co/GItRvCjABZ',\n",
       " u'#SM Quiz: Who Said It: Donald Trump or George Costanza?: Last week, Republican candidate\\u2026 https://t.co/fAjKFWKclP',\n",
       " u'Stephen Colbert Sets Election Night Live #Showtime Special: Stephen Colbert has finalized\\u2026 https://t.co/xaLZmfzovQ',\n",
       " u'Bir @YouTube videosu be\\u011fendim: https://t.co/v0tFmUcR8f Donald Trump (White Version)']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import twitter_trawling as twitter\n",
    "twitter_search=twitter.Twitter_Access()\n",
    "twitter_search.search_twitter('Donald Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of global warming was created by and for the Chinese in order to make US manufacturing non-competitive.\n",
      "['Chinese']\n",
      "This TPP sets the gold standard in trade agreements to open free, transparent, fair trade, the kind of environment that has the rule of law and a level playing field.\n",
      "[]\n",
      "The Obama administration has doubled the US national debt in eight years.\n",
      "[]\n",
      "The USA won the most medals at the 2016 Rio Olympics.\n",
      "[]\n",
      "The US employment rate is rising.\n",
      "[]\n",
      "The US population is 320 million.\n",
      "[]\n",
      "Donald Trump did not support the Iraq War.\n",
      "['Iraq']\n",
      "President Obama signed an executive order banning the Pledge of Allegiance in public schools.\n",
      "['Allegiance']\n",
      "Corey Lewandowski is Donald Trump’s campaign manager.\n",
      "[]\n",
      "Hillary Clinton is the Democratic candidate for President.\n",
      "[]\n",
      "Donald Trump \"was one of the people who rooted for the housing crisis. He said back in 2006, ‘Gee, I hope it does collapse because then I can go in and buy some and make some money.’\n",
      "[]\n",
      "USA has the lowest self employment rate in the OECD countries.\n",
      "[]\n",
      "The Forbes billionaires are richer than Japan.\n",
      "['Japan']\n",
      "Income inequality is 0.396 in the United States.\n",
      "['Income', 'United']\n",
      "There are more unemployed white women than there are unemployed black women in the US workforce.\n",
      "[]\n",
      "White women are nearly half as likely to be unemployed than black women.\n",
      "['White']\n",
      "Rep. Betty McCollum voted to pass the Water Resources Reform and Development Act of 2014.\n",
      "[]\n",
      "Barack Obama sponsored the Mercury Export Ban Act of 2008.\n",
      "[]\n",
      "Angelina Jolie and Brad Pitt are married.\n",
      "['Angelina']\n",
      "Donald Trump is the next president of the United States of America.\n",
      "['United', 'America']\n",
      "Texas has 37 electoral votes.\n",
      "['Texas']\n",
      " Sen. Dianne Feinstein is the chairman for the Senate Select Committee on Intelligence.\n",
      "[]\n",
      "5,000 workers earned less than minimum wage in Hawaii, in 2013\n",
      "['Hawaii']\n",
      "In Fairbanks Alaska there were 2017 farms in 2012 \n",
      "['Fairbanks']\n",
      "There are 3 hospital beds per 1,000 people\n",
      "[]\n",
      "In 2015 the USA produced 294,000 tonnes of carrots\n",
      "[]\n",
      "When I was secretary of state we increased american exports globally 30%\n",
      "[]\n",
      "5.0% of labor force is unemployed\n",
      "[]\n",
      "In Westchester, New York, the average wage is almost 10 times the national average.\n",
      "['Westchester', 'New']\n",
      "we know that having millions of people in the criminal justice system, without any ability to find a job after release, is unsustainable.  It’s bad for communities and it’s bad for our economy\n",
      "[]\n",
      "“I gratefully accept your nomination for the presidency of the United States\n",
      "['United']\n",
      "In his acceptance speech Mike Pence said “This is the legacy of Hillary Clinton: death, destruction and weakness.”\n",
      "[]\n",
      "Houston is the most diverse city in America\n",
      "['Houston', 'America']\n",
      "The President talked with the Ukrainian Deputy Prime Minister Volodymyr Groysman on May 13th 2016\n",
      "['Ukrainian']\n",
      "Spain has the most unemployed young men\n",
      "['Spain']\n",
      "Warren Buffett's 2015 tax deductions totalled almost $105.5m\n",
      "[]\n",
      "On average each person is the US is responsible for 16.2 tonnes of CO2 emissions\n",
      "[]\n",
      "In 1984 there were 682\n",
      "[]\n",
      "Hillary Clinton voted to raise taxes on workers earning as little as $41\n",
      "[]\n",
      "Heineken sponsors dog fighting events\n",
      "['Heineken']\n",
      "Louisiana Sen. Mary Landrieu \"received almost $1.8 million from BP over the last decade.\"\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import text_analysis as ta\n",
    "import csv\n",
    "analyser=ta.Text_Analyser()\n",
    "with open('sampledata.csv', 'rb') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        print row[3]\n",
    "        print(analyser.extract_entities(str(row[3])))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
